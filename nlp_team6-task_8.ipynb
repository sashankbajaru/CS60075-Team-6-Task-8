{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_team6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNx-fML30nZz"
      },
      "source": [
        "!pip install sklearn-crfsuite\n",
        "!pip install quantities\n",
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install cookiecutter\n",
        "!pip install stanza\n",
        "!pip install scispacy==0.3.0\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz\n",
        "!pip install pytorch-crf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVRziQhsRT49"
      },
      "source": [
        "Subtask-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUPccvvHkoa-"
      },
      "source": [
        "import spacy\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import chain\n",
        "\n",
        "import nltk\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from quantities import units as u\n",
        "\n",
        "data_path = \"drive/MyDrive/MeasEval-main/data/train/tsv\"\n",
        "textdata_path = \"drive/MyDrive/MeasEval-main/data/train/text\"\n",
        "test_data_path = \"drive/MyDrive/MeasEval-main/data/eval/tsv\"\n",
        "test_textdata_path = \"drive/MyDrive/MeasEval-main/data/eval/text\"\n",
        "\n",
        "\n",
        "units = ['%','â€°']\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "for key, val in u.__dict__.items():\n",
        "    if isinstance(val, type(u.l)):\n",
        "        if key not in units and key.lower() not in nlp.Defaults.stop_words:\n",
        "            units.append(key.lower())\n",
        "\n",
        "        if val.name not in units and val.name.lower() not in nlp.Defaults.stop_words:\n",
        "            units.append(val.name.lower())\n",
        "\n",
        "print(units)\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    lemma = sent[i][3]\n",
        "    dep = sent[i][4]\n",
        "    tag = sent[i][5]\n",
        "    is_stop = sent[i][6]\n",
        "    shape = sent[i][7]\n",
        "    token = sent[i][8]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'bais1': 1.0,\n",
        "        'word.lemma': lemma,\n",
        "        'word.is_unit': (word in units) or (lemma in units),\n",
        "        'word.shape': shape,\n",
        "        'word.dep': dep,\n",
        "        'word.is_stop()': is_stop,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.like_num': token.like_num,\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        lemma1 = sent[i-1][3]\n",
        "        dep1 = sent[i-1][4]\n",
        "        tag1 = sent[i-1][5]\n",
        "        is_stop1 = sent[i-1][6]\n",
        "        shape1 = sent[i-1][7]\n",
        "        token1 = sent[i-1][8]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.is_unit': (word1 in units) or (lemma1 in units),\n",
        "            '-1:word.lemma': lemma1,\n",
        "            '-1:word.shape': shape1,\n",
        "            '-1:word.dep': dep1,\n",
        "            '-1:word.is_stop()': is_stop1,\n",
        "            '-1:word.like_num': token1.like_num,\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:word.isdigit()': word1.isdigit(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i > 1:\n",
        "        word1 = sent[i-2][0]\n",
        "        postag1 = sent[i-2][1]\n",
        "        lemma1 = sent[i-2][3]\n",
        "        dep1 = sent[i-2][4]\n",
        "        tag1 = sent[i-2][5]\n",
        "        is_stop1 = sent[i-2][6]\n",
        "        shape1 = sent[i-2][7]\n",
        "        token1 = sent[i-2][8]\n",
        "        features.update({\n",
        "            '-2:word.lower()': word1.lower(),\n",
        "            '-2:word.lemma': lemma1,\n",
        "            '-2:word.shape': shape1,\n",
        "            '-2:word.dep': dep1,\n",
        "            '-2:word.like_num': token1.like_num,\n",
        "            '-2:word.istitle()': word1.istitle(),\n",
        "            '-2:word.isupper()': word1.isupper(),\n",
        "            '-2:word.isdigit()': word1.isdigit(),\n",
        "            '-2:postag': postag1,\n",
        "            '-2:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['SBOS'] = True\n",
        "\n",
        "\n",
        "    if i < len(sent)-2:\n",
        "        word1 = sent[i+2][0]\n",
        "        postag1 = sent[i+2][1]\n",
        "        lemma1 = sent[i+2][3]\n",
        "        dep1 = sent[i+2][4]\n",
        "        tag1 = sent[i+2][5]\n",
        "        is_stop1 = sent[i+2][6]\n",
        "        shape1 = sent[i+2][7]\n",
        "        token1 = sent[i+2][8]\n",
        "        features.update({\n",
        "            '+2:word.lower()': word1.lower(),\n",
        "            '+2:word.lemma': lemma1,\n",
        "            '+2:word.shape': shape1,\n",
        "            '+2:word.dep': dep1,\n",
        "            '+2:word.like_num': token1.like_num,\n",
        "            '+2:word.istitle()': word1.istitle(),\n",
        "            '+2:word.isupper()': word1.isupper(),\n",
        "            '+2:word.isdigit()': word1.isdigit(),\n",
        "            '+2:postag': postag1,\n",
        "            '+2:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['SEOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        lemma1 = sent[i+1][3]\n",
        "        dep1 = sent[i+1][4]\n",
        "        tag1 = sent[i+1][5]\n",
        "        is_stop1 = sent[i+1][6]\n",
        "        shape1 = sent[i+1][7]\n",
        "        token1 = sent[i+1][8]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.is_unit': (word1 in units) or (lemma1 in units),\n",
        "            '+1:word.lemma': lemma1,\n",
        "            '+1:word.lemma': lemma1,\n",
        "            '+1:word.shape': shape1,\n",
        "            '+1:word.dep': dep1,\n",
        "            '+1:word.is_stop()': is_stop1,\n",
        "            '+1:word.like_num': token1.like_num,\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:word.isdigit()': word1.isdigit(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label, lemma, dep, tag, is_stop, shape, token in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label, lemma, dep, tag, is_stop, shape, token in sent]\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sents = []\n",
        "train_sents = []\n",
        "test_sents = []\n",
        "labels = []\n",
        "features = []\n",
        "\n",
        "file_list = glob.glob(textdata_path+\"/*\")\n",
        "train_file_size = len(file_list)\n",
        "file_list+=glob.glob(test_textdata_path+\"/*\")\n",
        "it = 0\n",
        "\n",
        "test_file_list=[]\n",
        "for files in file_list:\n",
        "    file_obj = open(files)\n",
        "    if(it<train_file_size):\n",
        "      tsv_file = data_path+files[files.rfind('/'):-3]+\"tsv\" # the corresponding tsv file name\n",
        "    else:\n",
        "      tsv_file = test_data_path+files[files.rfind('/'):-3]+\"tsv\"\n",
        "    it+=1\n",
        "    #print(tsv_file)\n",
        "    try:\n",
        "      tsv_data = pd.read_csv(tsv_file, sep='\\t')\n",
        "    except:\n",
        "      print('file not found!')\n",
        "      continue\n",
        "        \n",
        "    s = file_obj.read()\n",
        "    tokens = nlp(s)\n",
        "    sent = []            # each sent contains each text file as an element\n",
        "\n",
        "    # Below lists are used to store the quantity spans from the tsv file\n",
        "    ent_spans = []\n",
        "    quant_spans = []\n",
        "    prop_spans = []\n",
        "\n",
        "    for k in range(0,tsv_data.shape[0]):\n",
        "      if(tsv_data.loc[k][2]=='Quantity' and tsv_data.loc[k][3] not in quant_spans):\n",
        "        quant_spans.append([tsv_data.loc[k][3],'s'])\n",
        "        quant_spans.append([tsv_data.loc[k][4],'e'])\n",
        "      if(tsv_data.loc[k][2]=='MeasuredEntity' and tsv_data.loc[k][3] not in ent_spans):\n",
        "        ent_spans.append([tsv_data.loc[k][3],'s'])\n",
        "        ent_spans.append([tsv_data.loc[k][4],'e'])\n",
        "      if(tsv_data.loc[k][2]=='MeasuredProperty' and tsv_data.loc[k][3] not in prop_spans):\n",
        "        prop_spans.append([tsv_data.loc[k][3],'s'])\n",
        "        prop_spans.append([tsv_data.loc[k][4],'e'])\n",
        "\n",
        "    ind = 0 #the index of the current word(token)\n",
        "    quant_end = 0 #stores the end of the current quantity span\n",
        "    ent_end = 0\n",
        "    prop_end = 0\n",
        "    for token in tokens:\n",
        "      word = token.text\n",
        "      shape = token.shape_\n",
        "      pos = token.pos_\n",
        "      lemma = token.lemma_\n",
        "      tag = token.tag_\n",
        "      dep = token.dep_\n",
        "      is_stop = token.is_stop\n",
        "      label = 'O'\n",
        "\n",
        "      ind = s.find(word,ind)\n",
        "      if ([ind,'s'] in quant_spans or ind < quant_end):\n",
        "        if([ind,'s'] in quant_spans):\n",
        "          quant_end = quant_spans[quant_spans.index([ind,'s'])+1][0]\n",
        "          label = 'B'\n",
        "          #print('quant: ',word)\n",
        "        else:\n",
        "          label = 'I'\n",
        "          #print('quant: ',word)\n",
        "      if ([ind,'s'] in ent_spans or ind < ent_end):\n",
        "        if([ind,'s'] in ent_spans):\n",
        "          ent_end = ent_spans[ent_spans.index([ind,'s'])+1][0]\n",
        "          #label = 'B1'\n",
        "          #print('ent: ',word)\n",
        "        #else:\n",
        "          #label = 'I1'\n",
        "          #print('ent: ',word)\n",
        "      if ([ind,'s'] in prop_spans or ind < prop_end):\n",
        "        if([ind,'s'] in prop_spans):\n",
        "          prop_end = prop_spans[prop_spans.index([ind,'s'])+1][0]\n",
        "          #label = 'B2'\n",
        "          #print('prop: ',word)\n",
        "        #else:\n",
        "          #label = 'I2'\n",
        "          #print('prop: ',word)\n",
        "      ind = ind + len(word)\n",
        "      sent.append((word,token.pos_,label,token.lemma_,token.dep_,token.tag_,token.is_stop,token.shape_,token)) \n",
        "    sents.append(sent)\n",
        "\n",
        "\n",
        "    if(it<=train_file_size):\n",
        "      train_sents.append(sent)\n",
        "    else:\n",
        "      test_file_list.append(files)\n",
        "      test_sents.append(sent)               \n",
        "#train_sents,test_sents = sklearn.model_selection.train_test_split(train_sents,test_size=0.30)\n",
        "\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    # other algorithms -lbfgs (has c1 also), l2sgd, ap, pa, arow \n",
        "    c2 = 0.1,\n",
        "    c1 = 0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)\n",
        "\n",
        "labels = list(crf.classes_)\n",
        "#print(labels)\n",
        "labels.remove('O')\n",
        "#labels.remove('B1')\n",
        "#labels.remove('I1')\n",
        "#labels.remove('B2')\n",
        "#labels.remove('I2')\n",
        "#print('size of labels: ',len(labels))\n",
        "\n",
        "y_pred = crf.predict(X_test)\n",
        "print(y_pred[0])\n",
        "print(y_pred[1])\n",
        "test_sents = list(test_sents)\n",
        "\n",
        "k = 0\n",
        "for i in range(0,len(test_sents)):\n",
        "  for j in range(0,len(test_sents[i])):\n",
        "    k+=1\n",
        "  if(k>4000):\n",
        "    break\n",
        "sc = metrics.flat_f1_score(y_test, y_pred,average='weighted', labels=labels)\n",
        "print(sc)\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n",
        "# state features\n",
        "\n",
        "from collections import Counter\n",
        "def print_state_features(state_features):\n",
        "    for (attr, label), weight in state_features:\n",
        "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
        "\n",
        "print(\"Top positive:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common(30))\n",
        "\n",
        "print(\"\\nTop negative:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common()[-30:])\n",
        "#print(len(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9uCnx8ZETvP"
      },
      "source": [
        "Creating the files in specified location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiiYglLMApHz",
        "outputId": "56082f5c-bb96-4211-ca5a-8846dd024528"
      },
      "source": [
        "import csv\n",
        "\n",
        "n=len(y_pred)\n",
        "default=\"{\\\"unit\\\": \\\"kg\\\"}\"\n",
        "default1=\"{\\\"HasQuantity\\\": \\\"T1-1\\\"}\"\n",
        "default2=\"{\\\"HasQuantity\\\": \\\"T1-1\\\"}\"\n",
        "csvFile = ''\n",
        "csvWriter = ''\n",
        "max_annot = []\n",
        "for i in range(n):\n",
        "  annt=1\n",
        "  max_annot.append(0)\n",
        "  file_obj = open(test_file_list[i])\n",
        "  #print(test_file_list[i])\n",
        "  if(test_file_list[i].find('S0012821X13007309-1649')+1>0):\n",
        "    print(y_pred[i])\n",
        "  s=file_obj.read()\n",
        "  csvFile = open(\"drive/MyDrive/MeasEval-main/eval/submission/\"+test_file_list[i][43:-4]+\".tsv\", 'w', newline='', encoding='utf8')\n",
        "  csvWriter = csv.writer(csvFile,delimiter='\\t')\n",
        "  csvWriter.writerow(['docId','annotSet','annotType','startOffset','endOffset','annotId','text','other'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtffE_ZtEXH1"
      },
      "source": [
        "writing the results of subtask-1 in the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK0Q5DOQsLhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f400ad4b-f8eb-4b6a-d7b9-b5dfaacc3552"
      },
      "source": [
        "import csv\n",
        "\n",
        "n=len(y_pred)\n",
        "default=\"{\\\"unit\\\": \\\"kg\\\"}\"\n",
        "default1=\"{\\\"HasQuantity\\\": \\\"T1-1\\\"}\"\n",
        "default2=\"{\\\"HasQuantity\\\": \\\"T1-1\\\"}\"\n",
        "csvFile = ''\n",
        "csvWriter = ''\n",
        "max_annot = []\n",
        "for i in range(n):\n",
        "  annt=1\n",
        "  max_annot.append(0)\n",
        "  file_obj = open(test_file_list[i])\n",
        "  #print(test_file_list[i])\n",
        "  if(test_file_list[i].find('S0012821X13007309-1649')+1>0):\n",
        "    print(y_pred[i])\n",
        "  s=file_obj.read()\n",
        "  csvFile = open(\"drive/MyDrive/MeasEval-main/eval/submission/\"+test_file_list[i][43:-4]+\".tsv\", 'a', newline='', encoding='utf8')\n",
        "  csvWriter = csv.writer(csvFile,delimiter='\\t')\n",
        "  #csvWriter.writerow(['docId','annotSet','annotType','startOffset','endOffset','annotId','text','other'])\n",
        "  words=[]\n",
        "  for j in range(len(test_sents[i])):\n",
        "    words.append(test_sents[i][j][0])\n",
        "  predictions=y_pred[i]\n",
        "  j=0\n",
        "  while j<len(predictions):\n",
        "    if predictions[j]!=\"B\":\n",
        "      j+=1\n",
        "      continue\n",
        "    else:\n",
        "      k=j+1\n",
        "      while k<len(predictions) and predictions[k]!=\"O\":\n",
        "        k+=1\n",
        "      ind=0\n",
        "      for t in range(j):\n",
        "        ind=s.find(words[t],ind)\n",
        "        ind+=len(words[t])\n",
        "      startoffset=s.find(words[j],ind)\n",
        "      ind=0\n",
        "      for t in range(k-1):\n",
        "        ind=s.find(words[t],ind)\n",
        "        ind+=len(words[t])\n",
        "      endoffset=s.find(words[k-1],ind)\n",
        "      endoffset+=len(words[k-1])\n",
        "      csvWriter.writerow([test_file_list[i][43:-4],annt,\"Quantity\",startoffset,endoffset,\"T1-\"+str(annt),s[startoffset:endoffset],default])\n",
        "      max_annot[i] = max(max_annot[i],annt)\n",
        "      annt+=1\n",
        "      j=k\n",
        "csvWriter = csv.writer(csvFile,delimiter='\\t')\n",
        "csvWriter.writerow(['docId','annotSet','annotType','startOffset','endOffset','annotId','text','other'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ-92j3t6oJQ"
      },
      "source": [
        "For testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwQeW3nhK9jM"
      },
      "source": [
        "!pwd\n",
        "!cd drive/MyDrive/MeasEval-main/eval\n",
        "!pip install -r drive/MyDrive/MeasEval-main/eval/requirements.txt\n",
        "!python drive/MyDrive/MeasEval-main/eval/measeval-eval.py -i drive/MyDrive/MeasEval-main/ -s eval/submission/ -g data/eval/tsv/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzik79vqlXey"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ups2FjL7RqMr",
        "outputId": "97e70afe-fe8c-41f0-8e45-f0c7278905e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxRskeW-b0nE"
      },
      "source": [
        "Subtask-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JbTshpm6nA1"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForPreTraining\n",
        "from sklearn.model_selection import train_test_split\n",
        "import stanza\n",
        "import spacy\n",
        "import re\n",
        "import en_core_sci_sm\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "from torchcrf import CRF\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfZb2xpkGAyu"
      },
      "source": [
        "data_path = \"drive/MyDrive/MeasEval-main/data/train/tsv\"\n",
        "textdata_path = \"drive/MyDrive/MeasEval-main/data/train/text\"\n",
        "test_data_path = \"drive/MyDrive/MeasEval-main/eval/submission\" # the one obtained in subtask-1 above\n",
        "test_textdata_path = \"drive/MyDrive/MeasEval-main/data/eval/text\""
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-Xa8rDLEki1"
      },
      "source": [
        "checking the device. GPU should be used for further code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlgT-dJW8TXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc73f17-0c22-4e02-bf4d-f81c28b866fa"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "print(device)\n",
        "device = torch.device(device)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmU6M07A8gIz"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VAwP5IyErg7"
      },
      "source": [
        "splitting sentence into bert tokens and storing their corresponding entity and property spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U4u_7Yd8noO"
      },
      "source": [
        "def sen_split(text):\n",
        "  doc = nlp(text)\n",
        "  sen = [s.text for s in doc.sents]\n",
        "  return sen\n",
        "\n",
        "file_list = glob.glob(textdata_path+\"/*\")\n",
        "train_file_size = len(file_list)\n",
        "file_list+=glob.glob(test_textdata_path+\"/*\")\n",
        "it = 0\n",
        "tuples = []  #stores the (sentence,quantity,measured entity,measured property) tuples (if they exist)\n",
        "test_tuples = [] #for the test set\n",
        "nlp1 = en_core_sci_sm.load()\n",
        "sum = 0\n",
        "\n",
        "for files in file_list:\n",
        "    file_obj = open(files)\n",
        "    if(it<train_file_size):\n",
        "      tsv_file = data_path+files[files.rfind('/'):-3]+\"tsv\" # the corresponding tsv file name\n",
        "    else:\n",
        "      tsv_file = test_data_path+files[files.rfind('/'):-3]+\"tsv\"\n",
        "    it+=1\n",
        "    s_file_name = files[files.rfind('/')+1:-4]\n",
        "    print(tsv_file)\n",
        "    try:\n",
        "      tsv_data = pd.read_csv(tsv_file, sep='\\t')\n",
        "    except:\n",
        "      print('file not found!')\n",
        "      continue\n",
        "\n",
        "    tsv_list = tsv_data.values\n",
        "    s = file_obj.read()\n",
        "    #tokens = nlp(s)\n",
        "    sent = []            # each sent contains each text file as an element\n",
        "\n",
        "    # Below lists are used to store the quantity spans from the tsv file\n",
        "    ent_spans = []\n",
        "    quant_spans = []\n",
        "    prop_spans = []\n",
        "    for k in range(0,tsv_data.shape[0]):\n",
        "      if(tsv_data.loc[k][2]=='Quantity' and tsv_data.loc[k][3] not in quant_spans):\n",
        "        sum+=1\n",
        "        quant_spans.append([tsv_data.loc[k][3],'s'])\n",
        "        quant_spans.append([tsv_data.loc[k][4],'e'])\n",
        "      if(tsv_data.loc[k][2]=='MeasuredEntity' and tsv_data.loc[k][3] not in ent_spans):\n",
        "        ent_spans.append([tsv_data.loc[k][3],'s'])\n",
        "        ent_spans.append([tsv_data.loc[k][4],'e'])\n",
        "      if(tsv_data.loc[k][2]=='MeasuredProperty' and tsv_data.loc[k][3] not in prop_spans):\n",
        "        prop_spans.append([tsv_data.loc[k][3],'s'])\n",
        "        prop_spans.append([tsv_data.loc[k][4],'e'])\n",
        "    \n",
        "    doc = nlp1(s)\n",
        "    sent_list = [s1.text for s1 in doc.sents]\n",
        "    ind = 0\n",
        "    ind1 = 0\n",
        "    \n",
        "    for sent in sent_list:\n",
        "      #ind1 = ind\n",
        "      for i in range(0,len(tsv_list)):\n",
        "        if (tsv_list[i][2]=='Quantity' and tsv_list[i][3]>=ind and tsv_list[i][4]<=ind+len(sent)):\n",
        "          val = []\n",
        "          val.append(sent)\n",
        "          val.append((tsv_list[i][3]-ind,tsv_list[i][4]-ind))\n",
        "          val.append((-1,-1))\n",
        "          val.append((-1,-1))\n",
        "          annotId = tsv_list[i][5]\n",
        "          val.append(annotId)\n",
        "          val.append(ind1)\n",
        "          val.append(s_file_name)\n",
        "          val.append(files)\n",
        "          val.append(tsv_list[i][1])\n",
        "          for j in range(i,len(tsv_list)):\n",
        "            if(tsv_list[j][2]=='MeasuredEntity' and tsv_list[j][4]<=ind+len(sent) and tsv_list[j][3]>=ind and tsv_list[j][1]==tsv_list[i][1]):\n",
        "              val[2] = (tsv_list[j][3]-ind,tsv_list[j][4]-ind)\n",
        "            if(tsv_list[j][2]=='MeasuredProperty' and tsv_list[j][3]>=ind and tsv_list[j][4]<=ind+len(sent) and tsv_list[j][1]==tsv_list[i][1]):\n",
        "              val[3] = (tsv_list[j][3]-ind,tsv_list[j][4]-ind)\n",
        "          if (it<=train_file_size):\n",
        "            tuples.append(val)\n",
        "          else:\n",
        "            test_tuples.append(val)\n",
        "      ind+=len(sent)+1\n",
        "      ind1+=len(sent)+1\n",
        "for tup in test_tuples:\n",
        "  print(tup)\n",
        "print(len(tuples))\n",
        "print(sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5ydTrImFA4J"
      },
      "source": [
        "Storing the measrued entity labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SHmthp5q_pz"
      },
      "source": [
        "#only for measured entity\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "pretrain = []\n",
        "i = 0\n",
        "\n",
        "def token_len(s):\n",
        "  cnt = 0\n",
        "  for i in s:\n",
        "    if(i!='#'):\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "for tup in tuples:\n",
        "  quan_labels = []\n",
        "  ent_labels = []\n",
        "  sent_tok = []\n",
        "  sent = tup[0]\n",
        "  tokens = nlp(sent)\n",
        "  annotId = tup[4]\n",
        "  ind1 = tup[5]\n",
        "  s_file_name = tup[6]\n",
        "  file_name = tup[7]\n",
        "  annot_set = tup[8]\n",
        "  ind = 0\n",
        "  for token in tokens:\n",
        "    word = token.text\n",
        "    ind = sent.find(word,ind)\n",
        "    bert_tokens = tokenizer.tokenize(word)\n",
        "    sent_tok.extend(bert_tokens)\n",
        "    for t in bert_tokens:\n",
        "      if (ind>=tup[1][0] and ind<=tup[1][1]):\n",
        "        quan_labels.append(1)\n",
        "      else:\n",
        "        quan_labels.append(0)\n",
        "      if (ind>=tup[2][0] and ind<=tup[2][1]):\n",
        "        ent_labels.append(1)\n",
        "      else:\n",
        "        ent_labels.append(0)\n",
        "      ind+=token_len(t)\n",
        "  for i in range(0,len(quan_labels)):\n",
        "    if quan_labels[i] == 1 and i == 0:\n",
        "        ent_labels.insert(i, 0)\n",
        "        sent_tok.insert(i,\"$\")\n",
        "    elif quan_labels[i] == 1 and quan_labels[i-1] == 0:\n",
        "      ent_labels.insert(i, 0)\n",
        "      sent_tok.insert(i,\"$\")\n",
        "    if quan_labels[i] == 1 and i == len(quan_labels) - 1:\n",
        "      ent_labels.insert(i+2, 0)\n",
        "      sent_tok.insert(i+2,\"$\")\n",
        "    elif quan_labels[i] == 1 and quan_labels[i+1] == 0:\n",
        "      ent_labels.insert(i+2, 0)\n",
        "      sent_tok.insert(i+2,\"$\")\n",
        "  pretrain.append([sent_tok, ent_labels,annotId,ind1,s_file_name,file_name,annot_set])\n",
        "\n",
        "validation = []\n",
        "for tup in test_tuples:\n",
        "  quan_labels = []\n",
        "  ent_labels = []\n",
        "  sent_tok = []\n",
        "  sent = tup[0]\n",
        "  tokens = nlp(sent)\n",
        "  tokens = nlp(sent)\n",
        "  annotId = tup[4]\n",
        "  ind1 = tup[5]\n",
        "  s_file_name = tup[6]\n",
        "  file_name = tup[7]\n",
        "  print(file_name)\n",
        "  annot_set = tup[8]\n",
        "  ind = 0\n",
        "  for token in tokens:\n",
        "    word = token.text\n",
        "    ind = sent.find(word,ind)\n",
        "    bert_tokens = tokenizer.tokenize(word)\n",
        "    sent_tok.extend(bert_tokens)\n",
        "    for t in bert_tokens:\n",
        "      if (ind>=tup[1][0] and ind<=tup[1][1]):\n",
        "        quan_labels.append(1)\n",
        "      else:\n",
        "        quan_labels.append(0)\n",
        "      if (ind>=tup[2][0] and ind<=tup[2][1]):\n",
        "        ent_labels.append(1)\n",
        "      else:\n",
        "        ent_labels.append(0)\n",
        "      ind+=token_len(t)\n",
        "  for i in range(0,len(quan_labels)):\n",
        "    if quan_labels[i] == 1 and i == 0:\n",
        "        ent_labels.insert(i, 0)\n",
        "        sent_tok.insert(i,\"$\")\n",
        "    elif quan_labels[i] == 1 and quan_labels[i-1] == 0:\n",
        "      ent_labels.insert(i, 0)\n",
        "      sent_tok.insert(i,\"$\")\n",
        "    if quan_labels[i] == 1 and i == len(quan_labels) - 1:\n",
        "      ent_labels.insert(i+2, 0)\n",
        "      sent_tok.insert(i+2,\"$\")\n",
        "    elif quan_labels[i] == 1 and quan_labels[i+1] == 0:\n",
        "      ent_labels.insert(i+2, 0)\n",
        "      sent_tok.insert(i+2,\"$\")\n",
        "  validation.append([sent_tok, ent_labels,annotId,ind1,s_file_name,file_name,annot_set])\n",
        "for ele in validation:\n",
        "  print((ele[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo5S2B7nFS2H"
      },
      "source": [
        "Storing the labels for measured quantity spans (skip the below cell when you want to run for measured entity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAGlrc3ynAbq"
      },
      "source": [
        "#only for measured property\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "pretrain = []\n",
        "i = 0\n",
        "\n",
        "def token_len(s):\n",
        "  cnt = 0\n",
        "  for i in s:\n",
        "    if(i!='#'):\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "for tup in tuples:\n",
        "  quan_labels = []\n",
        "  ent_labels = []\n",
        "  sent_tok = []\n",
        "  sent = tup[0]\n",
        "  tokens = nlp(sent)\n",
        "  annotId = tup[4]\n",
        "  ind1 = tup[5]\n",
        "  s_file_name = tup[6]\n",
        "  file_name = tup[7]\n",
        "  annot_set = tup[8]\n",
        "  ind = 0\n",
        "  for token in tokens:\n",
        "    word = token.text\n",
        "    ind = sent.find(word,ind)\n",
        "    bert_tokens = tokenizer.tokenize(word)\n",
        "    sent_tok.extend(bert_tokens)\n",
        "    for t in bert_tokens:\n",
        "      if (ind>=tup[1][0] and ind<=tup[1][1]):\n",
        "        quan_labels.append(1)\n",
        "      else:\n",
        "        quan_labels.append(0)\n",
        "      if (ind>=tup[3][0] and ind<=tup[3][1]):\n",
        "        ent_labels.append(1)\n",
        "      else:\n",
        "        ent_labels.append(0)\n",
        "      ind+=token_len(t)\n",
        "  for i in range(0,len(quan_labels)):\n",
        "    if quan_labels[i] == 1 and i == 0:\n",
        "        ent_labels.insert(i, 0)\n",
        "        sent_tok.insert(i,\"$\")\n",
        "    elif quan_labels[i] == 1 and quan_labels[i-1] == 0:\n",
        "      ent_labels.insert(i, 0)\n",
        "      sent_tok.insert(i,\"$\")\n",
        "    if quan_labels[i] == 1 and i == len(quan_labels) - 1:\n",
        "      ent_labels.insert(i+2, 0)\n",
        "      sent_tok.insert(i+2,\"$\")\n",
        "    elif quan_labels[i] == 1 and quan_labels[i+1] == 0:\n",
        "      ent_labels.insert(i+2, 0)\n",
        "      sent_tok.insert(i+2,\"$\")\n",
        "  pretrain.append([sent_tok, ent_labels,annotId,ind1,s_file_name,file_name,annot_set])\n",
        "\n",
        "validation = []\n",
        "for tup in test_tuples:\n",
        "  quan_labels = []\n",
        "  ent_labels = []\n",
        "  sent_tok = []\n",
        "  sent = tup[0]\n",
        "  tokens = nlp(sent)\n",
        "  tokens = nlp(sent)\n",
        "  annotId = tup[4]\n",
        "  ind1 = tup[5]\n",
        "  s_file_name = tup[6]\n",
        "  file_name = tup[7]\n",
        "  print(file_name)\n",
        "  annot_set = tup[8]\n",
        "  ind = 0\n",
        "  for token in tokens:\n",
        "    word = token.text\n",
        "    ind = sent.find(word,ind)\n",
        "    bert_tokens = tokenizer.tokenize(word)\n",
        "    sent_tok.extend(bert_tokens)\n",
        "    for t in bert_tokens:\n",
        "      if (ind>=tup[1][0] and ind<=tup[1][1]):\n",
        "        quan_labels.append(1)\n",
        "      else:\n",
        "        quan_labels.append(0)\n",
        "      if (ind>=tup[3][0] and ind<=tup[3][1]):\n",
        "        ent_labels.append(1)\n",
        "      else:\n",
        "        ent_labels.append(0)\n",
        "      ind+=token_len(t)\n",
        "  for i in range(0,len(quan_labels)):\n",
        "    if quan_labels[i] == 1 and i == 0:\n",
        "        ent_labels.insert(i, 0)\n",
        "        sent_tok.insert(i,\"$\")\n",
        "    elif quan_labels[i] == 1 and quan_labels[i-1] == 0:\n",
        "      ent_labels.insert(i, 0)\n",
        "      sent_tok.insert(i,\"$\")\n",
        "    if quan_labels[i] == 1 and i == len(quan_labels) - 1:\n",
        "      ent_labels.insert(i+2, 0)\n",
        "      sent_tok.insert(i+2,\"$\")\n",
        "    elif quan_labels[i] == 1 and quan_labels[i+1] == 0:\n",
        "      ent_labels.insert(i+2, 0)\n",
        "      sent_tok.insert(i+2,\"$\")\n",
        "  validation.append([sent_tok, ent_labels,annotId,ind1,s_file_name,file_name,annot_set])\n",
        "for ele in validation:\n",
        "  print((ele))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVgY41LMM0Tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48aeef78-254f-4ae7-cc67-7b5d933da78e"
      },
      "source": [
        "train = []\n",
        "x_train_id = np.zeros((0,256))\n",
        "x_train_mask = np.zeros((0,256))\n",
        "y_train = np.zeros((0,256))\n",
        "for val in pretrain:\n",
        "  tok_arr = [0]*256\n",
        "  att_mask = [0]*256\n",
        "  lab = [0]*256\n",
        "  tok_arr[0] = 102 # BERT token id of the [SEP] token\n",
        "  att_mask[0] = 1\n",
        "  for i in range(len(val[1])):\n",
        "    tok_arr[i+1] = tokenizer.convert_tokens_to_ids(val[0][i])\n",
        "    att_mask[i+1] = 1\n",
        "    lab[i+1] = val[1][i]\n",
        "  x_train_id = np.vstack((x_train_id, np.array(tok_arr)))\n",
        "  x_train_mask = np.vstack((x_train_mask, np.array(att_mask)))\n",
        "  y_train = np.vstack((y_train, np.array(lab)))\n",
        "  train.append([tok_arr,att_mask, lab])\n",
        "print(x_train_id.shape[0])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30woPJu4vTI5"
      },
      "source": [
        "val_data = []\n",
        "x_val_id = np.zeros((0,256))\n",
        "x_val_mask = np.zeros((0,256))\n",
        "y_val = np.zeros((0,256))\n",
        "for val in validation:\n",
        "  tok_arr = [0]*256\n",
        "  att_mask = [0]*256\n",
        "  lab = [0]*256\n",
        "  tok_arr[0] = 102 # BERT token id of the [SEP] token\n",
        "  att_mask[0] = 1\n",
        "  for i in range(len(val[1])):\n",
        "    tok_arr[i+1] = tokenizer.convert_tokens_to_ids(val[0][i])\n",
        "    att_mask[i+1] = 1\n",
        "    lab[i+1] = val[1][i]\n",
        "  x_val_id = np.vstack((x_val_id, np.array(tok_arr)))\n",
        "  x_val_mask = np.vstack((x_val_mask, np.array(att_mask)))\n",
        "  y_val = np.vstack((y_val, np.array(lab)))\n",
        "  train.append([tok_arr,att_mask, lab])\n",
        "print(x_val_mask[2])\n",
        "print(x_val_id.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlQ9CtrLUcVm"
      },
      "source": [
        "for i in range(len(y_train)):\n",
        "  for j in range(1,len(y_train[0])):\n",
        "    if y_train[i][j-1] == 0 and y_train[i][j] == 1:\n",
        "      y_train[i][j] = 2"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnT7rZcfvXSk"
      },
      "source": [
        "for i in range(len(y_val)):\n",
        "  for j in range(1,len(y_val[0])):\n",
        "    if y_val[i][j-1] == 0 and y_val[i][j] == 1:\n",
        "      y_val[i][j] = 2"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EI5uhGDUfUu"
      },
      "source": [
        "train_data = TensorDataset(torch.from_numpy(x_train_id), torch.from_numpy(x_train_mask), torch.from_numpy(y_train))\n",
        "val_data = TensorDataset(torch.from_numpy(x_val_id), torch.from_numpy(x_val_mask), torch.from_numpy(y_val))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KhFZInPUqMf"
      },
      "source": [
        "batch_size = 24\n",
        "train_loader = DataLoader(train_data, shuffle=False, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_data, shuffle=False, batch_size = batch_size)\n",
        "for x,y,z in val_loader:\n",
        "  print(z[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUgcPQfiUs-B"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyfXy-aAFdzY"
      },
      "source": [
        "Bert Architecture "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE7tXmn2Uxpv"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert, embed_dim, hidden_dim, drop_prob, n_layers, out_dim):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert \n",
        "      self.dropout = nn.Dropout(drop_prob)\n",
        "      self.fc1 = nn.Linear(2*embed_dim,out_dim)\n",
        "      self.w1 = nn.Linear(embed_dim, embed_dim)\n",
        "      self.w2 = nn.Linear(embed_dim, embed_dim)\n",
        "      #self.bilstm = nn.LSTM(embed_dim, hidden_dim,  bidirectional=True, batch_first=True)\n",
        "      self.softmax = nn.LogSoftmax(dim = 2)\n",
        "      self.crf = CRF(3, batch_first=True)  \n",
        "      self.tanh = nn.Tanh()\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask_val, labels=None):\n",
        "      x = self.bert(sent_id, attention_mask=mask_val)\n",
        "      x = x.last_hidden_state\n",
        "      x = self.tanh(x)\n",
        "      cls = x[:,0,:]\n",
        "      cls = cls.unsqueeze(1).repeat(1, 256, 1)\n",
        "      cls = self.w1(cls)\n",
        "      x = self.w2(x)\n",
        "      x = torch.cat([x,cls], dim = 2)\n",
        "      #x,_ = self.bilstm(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc1(x)\n",
        "      mask_val = mask_val.type(torch.uint8)\n",
        "      logit = self.softmax(x)\n",
        "      if labels is not None:\n",
        "          loss = -self.crf(logit, labels, mask=mask_val, reduction='mean')\n",
        "          return loss\n",
        "      else:\n",
        "          prediction = self.crf.decode(x, mask=mask_val)\n",
        "          return prediction"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO2ukghYU4O2"
      },
      "source": [
        "bert_model = BERT_Arch(model, 768, 64, 0.1, 1,3)\n",
        "bert_model = bert_model.to(device)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llkIvc8rU64I"
      },
      "source": [
        "print(bert_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NFkZVFLVBqM"
      },
      "source": [
        "optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMdJEZC8Fyh-"
      },
      "source": [
        "Training the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LaV2B_WTQA5"
      },
      "source": [
        "epochs = 10\n",
        "for e in range(epochs):\n",
        "  \n",
        "  bert_model.train()\n",
        "  i=0\n",
        "  train_loss=0\n",
        "  for seq, mask, y in train_loader:\n",
        "    bert_model.zero_grad()\n",
        "    loss = bert_model(seq.long().to(device), mask.long().to(device), y.long().to(device))\n",
        "    train_loss += loss.item()*batch_size\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    if(i%5==0):\n",
        "      print(\"Epoch-{}/{} Iterations-{} loss-{}\".format(e+1,epochs,i+1,loss.item()))\n",
        "    i+=1\n",
        "  \n",
        "  \n",
        "  bert_model.eval()\n",
        "  val_loss=0\n",
        "  for seq, mask, y in val_loader:\n",
        "    bert_model.zero_grad()\n",
        "    loss = bert_model(seq.long().to(device), mask.long().to(device), y.long().to(device))\n",
        "    val_loss += loss.item()*batch_size\n",
        "    i+=1\n",
        "  \n",
        "  print(\"Epoch-{}/{} train_loss-{} Val_loss-{}\".format(e+1,epochs,train_loss/len(train_loader),val_loss/len(val_loader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFnvPS7OFo0-"
      },
      "source": [
        "Creates measured entity entries in the output files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST71JeO4wBvR"
      },
      "source": [
        "#for measured entity\n",
        "import csv\n",
        "p=0\n",
        "n=0\n",
        "pos=0\n",
        "neg=0\n",
        "prec_num = 0 \n",
        "prec_den = 0\n",
        "rec_num = 0\n",
        "rec_den = 0 \n",
        "p1 = 0\n",
        "n1=0\n",
        "k1 = 0\n",
        "\n",
        "def remove_hash(s):\n",
        "  out = ''\n",
        "  for i in s:\n",
        "    if(i!='#'):\n",
        "      out+=i\n",
        "  return out\n",
        "\n",
        "file_cnt = -1\n",
        "annot = 1\n",
        "print('hi')\n",
        "file_name = ''\n",
        "for seq, mask, y in val_loader:\n",
        "    bert_model.zero_grad()\n",
        "    bert_model.eval()\n",
        "    y_pred = bert_model(seq.long().to(device), mask.long().to(device))\n",
        "    print(y_pred)\n",
        "    np_out = np.zeros((len(y_pred),256, 1))\n",
        "    np_act = (y.cpu().data.numpy() >= 1).astype(int).reshape((len(y), len(y[0]),1))\n",
        "    for i in range(k1,min(len(validation),k1+24)):\n",
        "      sent_tuple = validation[i]\n",
        "      annotId = validation[i][2]\n",
        "      ind = validation[i][3]\n",
        "      s_file_name = validation[i][4]\n",
        "      file_name1 = file_name\n",
        "      file_name = validation[i][5]\n",
        "      annot_set = validation[i][6]\n",
        "      fl = open(file_name,'r')\n",
        "      if(file_name1!=file_name):\n",
        "        print('annot is 1')\n",
        "        annot = 1\n",
        "        file_cnt+=1\n",
        "      s = fl.read()\n",
        "      s = s.casefold()\n",
        "      f2 = open(\"drive/MyDrive/MeasEval-main/eval/submission/\"+s_file_name+\".tsv\",'r', newline='', encoding='utf8')\n",
        "      s2 = f2.read()\n",
        "      startOffset = -1\n",
        "      endOffset = -1\n",
        "      pos = ind\n",
        "      cnt = 0\n",
        "      in_flag = 0\n",
        "\n",
        "      tsv_data = pd.read_csv(\"drive/MyDrive/MeasEval-main/eval/submission/\"+s_file_name+\".tsv\", sep='\\t')\n",
        "      max_annot = 0\n",
        "      for it in range(0,tsv_data.shape[0]):\n",
        "        if(tsv_data.loc[it][2]=='Quantity'):\n",
        "          max_annot+=1\n",
        "      for words in validation[i][0]:\n",
        "        if(words == '$'or (words[0]=='[' and words[len(words)-1]==']')):\n",
        "          cnt+=1\n",
        "          continue\n",
        "        out = remove_hash(words)\n",
        "        if (int(y_pred[i-k1][cnt+1])>0 and in_flag==0):\n",
        "          in_flag = 1\n",
        "          startOffset = s.find(out,pos)\n",
        "          #pos = s.find(out,pos)\n",
        "          #print('startOffset: ',startOffset,s.find(out,pos),out,ind,s)\n",
        "        elif (int(y_pred[i-k1][cnt+1])==0 and in_flag==1):\n",
        "          endOffset = s.find(out,pos)\n",
        "          print('endOffset: ',startOffset,endOffset,pos,out)\n",
        "          in_flag = 0\n",
        "          print('len(s2): ',len(s2))\n",
        "        cnt+=1\n",
        "        pos1 = pos\n",
        "        pos = s.find(out,pos)\n",
        "        if(pos==-1):\n",
        "          pos = ind\n",
        "        if(pos>pos1+1):\n",
        "          pos=pos1+1\n",
        "        pos+=len(out)\n",
        "      if (endOffset<startOffset):\n",
        "        endOffset = pos\n",
        "      if(startOffset+1>0 and startOffset<endOffset and len(s2)>80):\n",
        "        csvFile = open(\"drive/MyDrive/MeasEval-main/eval/submission/\"+s_file_name+\".tsv\", 'a', newline='', encoding='utf8')\n",
        "        csvWriter = csv.writer(csvFile,delimiter='\\t')\n",
        "        csvWriter.writerow([s_file_name,annot_set,\"MeasuredEntity\",startOffset,endOffset,'T2-'+str(annot),s[startOffset:endOffset],\"{\\\"HasQuantity\\\": \\\"T1-\"+str(annot_set)+\"\\\"}\"])\n",
        "        annot+=1\n",
        "\n",
        "    k1+=24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ-DCp1FGF7E"
      },
      "source": [
        "For measured property"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6JjPRt6nbOJ"
      },
      "source": [
        "#for measured property\n",
        "import csv\n",
        "p=0\n",
        "n=0\n",
        "pos=0\n",
        "neg=0\n",
        "prec_num = 0 \n",
        "prec_den = 0\n",
        "rec_num = 0\n",
        "rec_den = 0 \n",
        "p1 = 0\n",
        "n1=0\n",
        "k1 = 0\n",
        "\n",
        "def remove_hash(s):\n",
        "  out = ''\n",
        "  for i in s:\n",
        "    if(i!='#'):\n",
        "      out+=i\n",
        "  return out\n",
        "\n",
        "file_cnt = -1\n",
        "annot = 1\n",
        "print('hi')\n",
        "file_name = ''\n",
        "for seq, mask, y in val_loader:\n",
        "    bert_model.zero_grad()\n",
        "    bert_model.eval()\n",
        "    y_pred = bert_model(seq.long().to(device), mask.long().to(device))\n",
        "    print(y_pred)\n",
        "    np_out = np.zeros((len(y_pred),256, 1))\n",
        "    np_act = (y.cpu().data.numpy() >= 1).astype(int).reshape((len(y), len(y[0]),1))\n",
        "    for i in range(k1,min(len(validation),k1+24)):\n",
        "      sent_tuple = validation[i]\n",
        "      annotId = validation[i][2]\n",
        "      ind = validation[i][3]\n",
        "      s_file_name = validation[i][4]\n",
        "      file_name1 = file_name\n",
        "      file_name = validation[i][5]\n",
        "      annot_set = validation[i][6]\n",
        "      fl = open(file_name,'r')\n",
        "      if(file_name1!=file_name):\n",
        "        print('annot is 1')\n",
        "        annot = 1\n",
        "        file_cnt+=1\n",
        "      s = fl.read()\n",
        "      s = s.casefold()\n",
        "      f2 = open(\"drive/MyDrive/MeasEval-main/eval/submission/\"+s_file_name+\".tsv\",'r', newline='', encoding='utf8')\n",
        "      s2 = f2.read()\n",
        "      startOffset = -1\n",
        "      endOffset = -1\n",
        "      pos = ind\n",
        "      cnt = 0\n",
        "      in_flag = 0\n",
        "\n",
        "      tsv_data = pd.read_csv(\"drive/MyDrive/MeasEval-main/eval/submission/\"+s_file_name+\".tsv\", sep='\\t')\n",
        "      max_annot = 0\n",
        "      for it in range(0,tsv_data.shape[0]):\n",
        "        if(tsv_data.loc[it][2]=='Quantity'):\n",
        "          max_annot+=1\n",
        "      for words in validation[i][0]:\n",
        "        if(words == '$'or (words[0]=='[' and words[len(words)-1]==']')):\n",
        "          cnt+=1\n",
        "          continue\n",
        "        out = remove_hash(words)\n",
        "        if (int(y_pred[i-k1][cnt+1])>0 and in_flag==0):\n",
        "          in_flag = 1\n",
        "          startOffset = s.find(out,pos)\n",
        "          #pos = s.find(out,pos)\n",
        "          #print('startOffset: ',startOffset,s.find(out,pos),out,ind,s)\n",
        "        elif (int(y_pred[i-k1][cnt+1])==0 and in_flag==1):\n",
        "          endOffset = s.find(out,pos)\n",
        "          #print('endOffset: ',startOffset,endOffset,pos,out)\n",
        "          in_flag = 0\n",
        "          #print('len(s2): ',len(s2))\n",
        "        cnt+=1\n",
        "        pos1 = pos\n",
        "        pos = s.find(out,pos)\n",
        "        if(pos==-1):\n",
        "          pos = ind\n",
        "        if(pos>pos1+1):\n",
        "          pos=pos1+1\n",
        "        pos+=len(out)\n",
        "      if (endOffset<startOffset):\n",
        "        endOffset = pos\n",
        "      if(startOffset+1>0 and startOffset<endOffset and len(s2)>80):\n",
        "        csvFile = open(\"drive/MyDrive/MeasEval-main/eval/submission/\"+s_file_name+\".tsv\", 'a', newline='', encoding='utf8')\n",
        "        csvWriter = csv.writer(csvFile,delimiter='\\t')\n",
        "        csvWriter.writerow([s_file_name,annot_set,\"MeasuredProperty\",startOffset,endOffset,'T3-'+str(annot),s[startOffset:endOffset],\"{\\\"HasQuantity\\\": \\\"T1-\"+str(annot_set)+\"\\\"}\"])\n",
        "        annot+=1\n",
        "\n",
        "    k1+=24"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}